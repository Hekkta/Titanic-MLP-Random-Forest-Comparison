# Titanic-MLP-Random-Forest-Comparison

This was my entry for an introductory competition given on Kaggle.

A training dataset is given with the target labels alongside a test dataset without the labels.

In the notebook I attempt to find the optimal choice of classifier by tweaking a primaryle in each and comparing the two.

The final score on the test data set was 0.78468 which is slightly less than the predicted accuracy in the notebook.
